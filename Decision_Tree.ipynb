{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "- A Decision Tree is a supervised model that recursively splits data based on feature-based rules. Each internal node checks a condition like “feature ≤ threshold”, dividing data into subsets with reduced impurity. This continues until leaves contain mostly a single class. For classification, the leaf’s majority class becomes the prediction. Decision Trees are interpretable and can handle nonlinear boundaries.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "- Gini Impurity calculates the likelihood of incorrect classification if labels were randomly assigned. Entropy measures uncertainty or disorder. Both impurities become zero when a node is perfectly pure. During tree construction, every split is evaluated, and the split that gives the highest impurity reduction is chosen. This makes nodes purer and improves classification accuracy.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "- Pre-pruning restricts tree growth early using parameters such as max_depth or min_samples_split, preventing overfitting and reducing training time. Post-pruning grows a full tree first and then removes unnecessary branches, improving generalization. Pre-pruning is faster; post-pruning produces more optimized trees.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "- Information Gain measures the reduction in entropy achieved after a split. A high Information Gain indicates that the split separates the classes effectively. The tree uses this value to choose the most informative features, improving prediction accuracy and reducing tree depth.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "- Decision Trees are used in fraud detection, medical diagnosis, credit scoring, churn prediction, and recommendation systems. They are easy to interpret and handle mixed data types. However, they can overfit and are sensitive to small data changes. Ensembles like Random Forests help overcome these issues."
      ],
      "metadata": {
        "id": "J5qrbW3z7Xeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "* Load the Iris Dataset\n",
        "* Train a Decision Tree Classifier using the Gini criterion\n",
        "* Print the model’s accuracy and feature importances\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IX90T_7u8MlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fGnIOxI7QkY",
        "outputId": "4e623286-b579-4ad8-fb6c-2b964058b456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8947368421052632\n",
            "Feature Importances:\n",
            "sepal length (cm) : 0.013393924898349679\n",
            "sepal width (cm) : 0.020090887347524518\n",
            "petal length (cm) : 0.9198866667893217\n",
            "petal width (cm) : 0.04662852096480414\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(\"Feature Importances:\")\n",
        "for name, imp in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(name, \":\", imp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "* Load the Iris Dataset\n",
        "* Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "TG297jQl8aCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "clf3.fit(X_train, y_train)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "pred3 = clf3.predict(X_test)\n",
        "pred_full = clf_full.predict(X_test)\n",
        "\n",
        "print(\"Accuracy (max_depth=3):\", accuracy_score(y_test, pred3))\n",
        "print(\"Accuracy (full tree):\", accuracy_score(y_test, pred_full))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjFAMh478XIa",
        "outputId": "2999023f-17d5-4052-8fb3-0072518e8ab6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 0.8947368421052632\n",
            "Accuracy (full tree): 0.8947368421052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "* Load the California Housing dataset from sklearn\n",
        "* Train a Decision Tree Regressor\n",
        "* Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "YXIGG3uL8m88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "pred = reg.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, pred))\n",
        "print(\"Feature Importances:\")\n",
        "for name, imp in zip(data.feature_names, reg.feature_importances_):\n",
        "    print(name, \":\", imp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtWK9nxy8iuK",
        "outputId": "89492b9c-9105-4a2d-d98a-980c20559b08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.5285224061284108\n",
            "Feature Importances:\n",
            "MedInc : 0.5262413969849339\n",
            "HouseAge : 0.050926206129984955\n",
            "AveRooms : 0.048154956928807016\n",
            "AveBedrms : 0.02803899237580427\n",
            "Population : 0.03691354728127817\n",
            "AveOccup : 0.13491387033351493\n",
            "Latitude : 0.08801244866407874\n",
            "Longitude : 0.08679858130159805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "* Load the Iris Dataset\n",
        "* Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "* Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "XdRI6GZk8y49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris # Import load_iris\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "\n",
        "# Load the Iris Dataset again to ensure correct data is used\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data again to ensure correct data is used\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "params = {\n",
        "    \"max_depth\": [None, 2, 3, 4, 5, 6],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=params,\n",
        "    cv=5,\n",
        "    n_jobs=-1)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUE_YZKc8ugV",
        "outputId": "fd12b5e4-31e2-4fbf-f407-c0cd6398c302"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 5}\n",
            "Accuracy: 0.9210526315789473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a97941d"
      },
      "source": [
        "## Question 10: Healthcare Disease Prediction with Decision Trees\n",
        "\n",
        "Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "\n",
        "*   Handle the missing values\n",
        "*   Encode the categorical features\n",
        "*   Train a Decision Tree model\n",
        "*   Tune its hyperparameters\n",
        "*   Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "### Step-by-Step Process:\n",
        "\n",
        "1.  **Data Loading and Initial Exploration:**\n",
        "    *   Load the dataset into a pandas DataFrame.\n",
        "    *   Perform initial data exploration to understand the structure, identify data types, and get a sense of the extent of missing values and categorical features.\n",
        "\n",
        "2.  **Handling Missing Values:**\n",
        "    *   **Identify Missing Values:** Determine which features have missing values and the percentage of missing data in each.\n",
        "    *   **Choose Imputation Strategy:** Select appropriate strategies based on the data type and distribution. Common methods include:\n",
        "        *   **Mean/Median Imputation:** For numerical features. Use the mean for normally distributed data and the median for skewed data.\n",
        "        *   **Mode Imputation:** For categorical features.\n",
        "        *   **K-Nearest Neighbors (KNN) Imputation:** Impute missing values based on the values of the k-nearest neighbors.\n",
        "        *   **Model-Based Imputation:** Use a model to predict missing values based on other features.\n",
        "    *   **Implement Imputation:** Apply the chosen imputation methods to fill in the missing values.\n",
        "\n",
        "3.  **Encoding Categorical Features:**\n",
        "    *   **Identify Categorical Features:** Determine which features are categorical (nominal or ordinal).\n",
        "    *   **Choose Encoding Strategy:** Select appropriate encoding methods:\n",
        "        *   **One-Hot Encoding:** For nominal features where there is no inherent order. Creates new binary columns for each category.\n",
        "        *   **Label Encoding:** For ordinal features where there is a natural order. Assigns a unique integer to each category. Be cautious with Decision Trees as they can misinterpret the numerical order if the feature is not truly ordinal.\n",
        "        *   **Target Encoding:** Encodes categories based on the mean of the target variable for each category. Can be useful but can also lead to overfitting.\n",
        "    *   **Implement Encoding:** Apply the chosen encoding methods to convert categorical features into a numerical format that the Decision Tree can understand.\n",
        "\n",
        "4.  **Splitting the Data:**\n",
        "    *   Split the dataset into training, validation (optional but recommended for hyperparameter tuning), and testing sets. A common split is 70-80% for training, 10-15% for validation, and 10-15% for testing. Ensure the split is stratified if the target variable is imbalanced to maintain the original class distribution in each set.\n",
        "\n",
        "5.  **Training a Decision Tree Model:**\n",
        "    *   Import the `DecisionTreeClassifier` from scikit-learn.\n",
        "    *   Instantiate the model. Start with default hyperparameters or values based on domain knowledge or initial exploration.\n",
        "    *   Train the model using the training data (`fit(X_train, y_train)`).\n",
        "\n",
        "6.  **Hyperparameter Tuning:**\n",
        "    *   **Identify Key Hyperparameters:** Focus on hyperparameters that control the complexity of the tree, such as:\n",
        "        *   `max_depth`: The maximum depth of the tree.\n",
        "        *   `min_samples_split`: The minimum number of samples required to split an internal node.\n",
        "        *   `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
        "        *   `criterion`: The function to measure the quality of a split (Gini impurity or entropy).\n",
        "    *   **Choose Tuning Method:**\n",
        "        *   **Grid Search:** Exhaustively searches over a specified range of hyperparameter values.\n",
        "        *   **Random Search:** Randomly samples hyperparameter values from a specified distribution. Often more efficient than grid search for large search spaces.\n",
        "        *   **Cross-Validation:** Use cross-validation (e.g., k-fold cross-validation) during tuning to get a more robust estimate of the model's performance for each set of hyperparameters.\n",
        "    *   **Implement Tuning:** Use `GridSearchCV` or `RandomizedSearchCV` from scikit-learn to find the best combination of hyperparameters based on a chosen evaluation metric (e.g., accuracy, precision, recall, F1-score, AUC, depending on the business problem and class imbalance).\n",
        "    *   **Select Best Model:** Choose the model with the best performance on the validation set (or cross-validation) as the final model.\n",
        "\n",
        "7.  **Evaluating Model Performance:**\n",
        "    *   Evaluate the performance of the best model on the unseen test set using appropriate evaluation metrics. For disease prediction, common metrics include:\n",
        "        *   **Accuracy:** Overall percentage of correct predictions.\n",
        "        *   **Precision:** Of all patients predicted to have the disease, what percentage actually have it? (Minimizes false positives).\n",
        "        *   **Recall (Sensitivity):** Of all patients who actually have the disease, what percentage were correctly identified? (Minimizes false negatives).\n",
        "        *   **F1-Score:** The harmonic mean of precision and recall, providing a balance between the two.\n",
        "        *   **AUC (Area Under the ROC Curve):** Measures the model's ability to distinguish between positive and negative classes.\n",
        "        *   **Confusion Matrix:** A table summarizing the prediction results, showing true positives, true negatives, false positives, and false negatives.\n",
        "    *   Analyze the results in the context of the business problem to understand the model's strengths and weaknesses.\n",
        "\n",
        "### Business Value in a Real-World Setting:\n",
        "\n",
        "A Decision Tree model for disease prediction in a healthcare company could provide significant business value:\n",
        "\n",
        "*   **Early Detection and Intervention:** Identifying patients at high risk of developing a disease allows for earlier intervention, potentially leading to better patient outcomes, reduced treatment costs, and improved quality of life.\n",
        "*   **Resource Allocation:** The model can help healthcare providers prioritize resources by identifying patients who require more immediate attention or specialized care.\n",
        "*   **Personalized Treatment Plans:** Understanding the factors that contribute to disease risk for individual patients can help in developing more personalized and effective treatment plans.\n",
        "*   **Cost Reduction:** By enabling early detection and targeted interventions, the model can help reduce the overall cost of healthcare by preventing the progression of diseases and minimizing the need for more expensive treatments later on.\n",
        "*   **Improved Patient Management:** The model can support healthcare professionals in making more informed decisions about patient management, leading to improved efficiency and effectiveness of care delivery.\n",
        "*   **Research and Insights:** The feature importances from the Decision Tree can provide valuable insights into the key factors associated with the disease, which can inform further research and understanding of the disease mechanisms.\n",
        "*   **Risk Stratification:** Patients can be stratified into different risk categories based on the model's predictions, allowing for tailored monitoring and preventative measures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsixGFtD84ku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}